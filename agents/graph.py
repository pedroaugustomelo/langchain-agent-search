import logging
from langgraph.graph import StateGraph, START, END
from langchain_aws import ChatBedrock
from agents.conversational_agent import conversational_agent
from agents.search_agent import web_search_agent
from agents.moderation_agent import moderation_agent
from utils.AgentState import AgentState
from utils.routings import moderation_routing, conversational_routing
from langgraph.checkpoint.memory import MemorySaver

def run_graph(user_input: str):
    """
    Executes the agent workflow, processing user input through moderation, conversational, and web search agents.
    
    Args:
        user_input (str): The user's input message.
    
    Returns:
        str: The final response message generated by the workflow.
    """
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("agent.log"),
            logging.StreamHandler()
        ]
    )
    
    agentState = AgentState()
    
    # Initialize the language model
    llm = ChatBedrock(model_id="anthropic.claude-3-haiku-20240307-v1:0", model_kwargs={"temperature": 0.9})
    
    # Define the agent workflow graph
    workflow = StateGraph(AgentState)
    
    workflow.add_node("conversational", lambda state: conversational_agent(llm, state))
    workflow.add_node("web_search", web_search_agent)
    workflow.add_node("moderation", lambda state: moderation_agent(llm, state))
    
    # Define workflow transitions
    workflow.add_edge(START, "moderation")
    workflow.add_conditional_edges(
        "moderation",
        moderation_routing,
        {"conversational": "conversational", END: END} 
    )
    workflow.add_conditional_edges(
        "conversational",
        conversational_routing,
        {"web_search": "web_search", "moderation": "moderation"}
    )
    workflow.add_conditional_edges(
        "web_search",
        lambda state: "conversational",
        {"conversational": "conversational"}
    )
    
    # Set up checkpointing
    checkpointer = MemorySaver()
    graph_executor = workflow.compile(checkpointer=checkpointer)
    
    # Initialize the conversation state
    messages = [{"role": "user", "content": user_input}]
    initial_state = AgentState(messages=messages, web_snippets=[], search_needed=False, search_attempted=False)
    
    logging.info("Starting the agent execution")
    
    # Execute the workflow
    final_output = graph_executor.invoke(initial_state, config={"configurable": {"thread_id": 3}})
    
    # Retrieve and return the final response
    state = dict(final_output)
    
    return state["messages"][-1]["content"]
